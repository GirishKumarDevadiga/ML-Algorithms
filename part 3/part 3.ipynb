{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:29:02.140023Z",
     "start_time": "2021-01-24T06:29:02.130250Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:29:04.084284Z",
     "start_time": "2021-01-24T06:29:04.017593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width           class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('iris.data', header = None, names = ['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the Python code to compute entropy and information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:29:09.480599Z",
     "start_time": "2021-01-24T06:29:07.247194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'petal length': {1.0: 'Iris-setosa',\n",
      "                  1.1: 'Iris-setosa',\n",
      "                  1.2: 'Iris-setosa',\n",
      "                  1.3: 'Iris-setosa',\n",
      "                  1.4: 'Iris-setosa',\n",
      "                  1.5: 'Iris-setosa',\n",
      "                  1.6: 'Iris-setosa',\n",
      "                  1.7: 'Iris-setosa',\n",
      "                  1.9: 'Iris-setosa',\n",
      "                  3.3: 'Iris-versicolor',\n",
      "                  3.5: 'Iris-versicolor',\n",
      "                  3.6: 'Iris-versicolor',\n",
      "                  3.9: 'Iris-versicolor',\n",
      "                  4.0: 'Iris-versicolor',\n",
      "                  4.1: 'Iris-versicolor',\n",
      "                  4.2: 'Iris-versicolor',\n",
      "                  4.3: 'Iris-versicolor',\n",
      "                  4.4: 'Iris-versicolor',\n",
      "                  4.5: 'Iris-versicolor',\n",
      "                  4.6: 'Iris-versicolor',\n",
      "                  4.7: 'Iris-versicolor',\n",
      "                  4.8: 'Iris-versicolor',\n",
      "                  4.9: 'Iris-versicolor',\n",
      "                  5.0: 'Iris-versicolor'}}\n",
      "The prediction accuracy is:  22.857142857142858 %\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "def entropy(target_col):  \n",
    "    \n",
    "    elements,counts = np.unique(target_col,return_counts = True)  \n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])  \n",
    "    return entropy  \n",
    "  \n",
    "def InfoGain(data,split_attribute_name,target_name=\"class\"):  \n",
    "         \n",
    "    #Calculate the entropy of the total dataset  \n",
    "    total_entropy = entropy(data[target_name])  \n",
    "      \n",
    "    ##Calculate the entropy of the dataset  \n",
    "      \n",
    "    #Calculate the values and the corresponding counts for the split attribute   \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)  \n",
    "      \n",
    "    #Calculate the weighted entropy  \n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])  \n",
    "      \n",
    "    #Calculate the information gain  \n",
    "    Information_Gain = total_entropy - Weighted_Entropy  \n",
    "    return Information_Gain  \n",
    "  \n",
    "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):  \n",
    "  \n",
    "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#  \n",
    "      \n",
    "    #If all target_values have the same value, return this value  \n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:  \n",
    "        return np.unique(data[target_attribute_name])[0]  \n",
    "      \n",
    "    #If the dataset is empty, return the mode target feature value in the original dataset  \n",
    "    elif len(data)==0:  \n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]  \n",
    "      \n",
    "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that  \n",
    "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence  \n",
    "    #the mode target feature value is stored in the parent_node_class variable.  \n",
    "      \n",
    "    elif len(features) ==0:  \n",
    "        return parent_node_class  \n",
    "      \n",
    "    #If none of the above holds true, grow the tree!  \n",
    "      \n",
    "    else:  \n",
    "        #Set the default value for this node --> The mode target feature value of the current node  \n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]  \n",
    "          \n",
    "        #Select the feature which best splits the dataset  \n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset  \n",
    "        best_feature_index = np.argmax(item_values)  \n",
    "        best_feature = features[best_feature_index]  \n",
    "          \n",
    "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information  \n",
    "        #gain in the first run  \n",
    "        tree = {best_feature:{}}  \n",
    "          \n",
    "          \n",
    "        #Remove the feature with the best inforamtion gain from the feature space  \n",
    "        features = [i for i in features if i != best_feature]  \n",
    "          \n",
    "        #Grow a branch under the root node for each possible value of the root node feature  \n",
    "          \n",
    "        for value in np.unique(data[best_feature]):  \n",
    "            value = value  \n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets  \n",
    "            sub_data = data.where(data[best_feature] == value).dropna()  \n",
    "              \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!  \n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)  \n",
    "              \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node  \n",
    "            tree[best_feature][value] = subtree  \n",
    "              \n",
    "        return(tree)      \n",
    "                  \n",
    "def predict(query,tree,default = 1):  \n",
    "    \n",
    "        for key in list(query.keys()):  \n",
    "            if key in list(tree.keys()):  \n",
    "\n",
    "                try:  \n",
    "                    result = tree[key][query[key]]   \n",
    "                except:  \n",
    "                    return default  \n",
    "\n",
    "                result = tree[key][query[key]]  \n",
    "\n",
    "                if isinstance(result,dict):  \n",
    "                    return predict(query,result)  \n",
    "                else:  \n",
    "                    return result  \n",
    "\n",
    "  \n",
    "def train_test_split(dataset):  \n",
    "        training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index  \n",
    "        #starting form 0, because we do not want to run into errors regarding the row labels / indexes  \n",
    "        testing_data = dataset.iloc[80:].reset_index(drop=True)  \n",
    "        return training_data,testing_data  \n",
    "  \n",
    "training_data = train_test_split(dataset)[0]  \n",
    "testing_data = train_test_split(dataset)[1]   \n",
    "  \n",
    "def test(data,tree):  \n",
    "        #Create new query instances by simply removing the target feature column from the original dataset and   \n",
    "        #convert it to a dictionary  \n",
    "        queries = data.iloc[:,:-1].to_dict(orient = \"records\")  \n",
    "\n",
    "        #Create a empty DataFrame in whose columns the prediction of the tree are stored  \n",
    "        predicted = pd.DataFrame(columns=[\"predicted\"])   \n",
    "\n",
    "        #Calculate the prediction accuracy  \n",
    "        for i in range(len(data)):  \n",
    "            predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0)   \n",
    "        print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')  \n",
    "      \n",
    "tree = ID3(training_data,training_data,training_data.columns[:-1])  \n",
    "pprint(tree)  \n",
    "test(testing_data,tree) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write the  Python code to demonstrate conditional probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:54:29.526328Z",
     "start_time": "2021-01-24T06:54:24.964207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers               : {20: 24972, 30: 24912, 40: 25082, 50: 25034}\n",
      "Customers that pucheased: {20: 4928, 30: 7516, 40: 10000, 50: 12471}\n",
      "\n",
      "Probability of purchase (PE) is 0.34915 \n",
      "\n",
      "\n",
      "PF for age 20 is 0.24972\n",
      "P(F|E) Probability of purchasing at age 20 is 0.19734102194457792\n",
      "P(E,F) at age: 20  is 0.04928\n",
      "\n",
      "PF for age 30 is 0.24912\n",
      "P(F|E) Probability of purchasing at age 30 is 0.3017019910083494\n",
      "P(E,F) at age: 30  is 0.07516\n",
      "\n",
      "PF for age 40 is 0.25082\n",
      "P(F|E) Probability of purchasing at age 40 is 0.3986922892911251\n",
      "P(E,F) at age: 40  is 0.09999999999999999\n",
      "\n",
      "PF for age 50 is 0.25034\n",
      "P(F|E) Probability of purchasing at age 50 is 0.49816249900135817\n",
      "P(E,F) at age: 50  is 0.12471\n"
     ]
    }
   ],
   "source": [
    "from numpy import random as rand\n",
    "\n",
    "#random seed generetor\n",
    "rand.seed(0)\n",
    "\n",
    "#list of age\n",
    "age_list=[20,30,40,50]\n",
    "\n",
    "#dict of age for any customer and purchasing customer\n",
    "cust= {age_list[0]:0, age_list[1]:0, age_list[2]:0, age_list[3]:0}\n",
    "purch= {age_list[0]:0, age_list[1]:0, age_list[2]:0, age_list[3]:0}\n",
    "\n",
    "total_purch=0\n",
    "\n",
    "#number of potential customers\n",
    "n=100000\n",
    "\n",
    "#counter for for loop\n",
    "counter = 0\n",
    "\n",
    "#sample data generetor\n",
    "for _ in range(n):\n",
    "    \n",
    "    #picks random age from list\n",
    "    age=rand.choice(age_list)\n",
    "    #adds one customer to the picked age group\n",
    "    cust[age] += 1\n",
    "    \n",
    "    #custom sample of the relation between customer age and purchase probability \n",
    "    prob_purch= float(age) / 100.0\n",
    "    if (rand.random() <+ prob_purch):\n",
    "        purch[age] += 1\n",
    "        total_purch += 1\n",
    "        \n",
    "#print result\n",
    "print(\"Customers               : \" + str(cust))\n",
    "print(\"Customers that pucheased: \" + str(purch))\n",
    "\n",
    "\n",
    "#probability of someone purchasing\n",
    "PE= float(total_purch) / n\n",
    "print(\"\\nProbability of purchase (PE) is \" + str(PE), \"\\n\")\n",
    "\n",
    "#conditional probability calculator\n",
    "for _ in range(4):\n",
    "    \n",
    "    #picks age from ahe list in order\n",
    "    age=age_list[counter]\n",
    "    counter += 1\n",
    "    \n",
    "    #probability of any an age from potential customers \n",
    "    PF=float(cust[age]) / n\n",
    "    print(\"\\nPF for age \" + str(age) + \" is \" + str(PF))\n",
    "    \n",
    "    #probability of being an specific age group and puchasing\n",
    "    PFE= float(purch[age]) / float(cust[age])\n",
    "    print(\"P(F|E) Probability of purchasing at age \" + str(age) + \" is \" + str(PFE))\n",
    "    \n",
    "    #probability of a specific age group perchasing out of all potential customers\n",
    "    PEF= PF * PFE\n",
    "    print(\"P(E,F) at age: \" + str(age), \" is \" + str(PEF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write the  Python code to compute Euclidean Distance between data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:56:08.446436Z",
     "start_time": "2021-01-24T06:56:08.369234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance from x to y:  4.69041575982343\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Example points in 3-dimensional space...\n",
    "x = (5, 6, 7)\n",
    "y = (8, 9, 9)\n",
    "distance = math.sqrt(sum([(a - b) ** 2 for a, b in zip(x, y)]))\n",
    "print(\"Euclidean distance from x to y: \",distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write the  Python code to calculate covariance matrix, Eigen values and Eigen vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T07:04:21.762158Z",
     "start_time": "2021-01-24T07:04:21.738012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the Original square array:\n",
      " [[1 2 3]\n",
      " [2 3 4]\n",
      " [4 5 6]]\n",
      "\n",
      "Printing the covariance matrix of the given square array:\n",
      " [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "Printing the Eigen values of the given square array:\n",
      " [ 1.08309519e+01 -8.30951895e-01  1.01486082e-16]\n",
      "\n",
      "Printing eigenvectors of the given square array:\n",
      " [[ 0.34416959  0.72770285  0.40824829]\n",
      " [ 0.49532111  0.27580256 -0.81649658]\n",
      " [ 0.79762415 -0.62799801  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "# importing numpy library \n",
    "import numpy as np \n",
    "  \n",
    "# create numpy 2d-array \n",
    "m = np.array([[1, 2, 3], \n",
    "              [2, 3, 4], \n",
    "              [4, 5, 6]]) \n",
    "  \n",
    "print(\"Printing the Original square array:\\n\", \n",
    "      m) \n",
    "print()\n",
    "# finding the covariance matrix\n",
    "c = np.cov(m)\n",
    "\n",
    "# printing covariance matrix \n",
    "print(\"Printing the covariance matrix of the given square array:\\n\", \n",
    "      c) \n",
    "print()  \n",
    "# finding eigenvalues and eigenvectors \n",
    "w, v = np.linalg.eig(m) \n",
    "  \n",
    "# printing eigen values \n",
    "print(\"Printing the Eigen values of the given square array:\\n\", \n",
    "      w) \n",
    "print()  \n",
    "# printing eigen vectors \n",
    "print(\"Printing eigenvectors of the given square array:\\n\", \n",
    "      v) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write the  Python code to calculate the following\n",
    "####     Accuracy\n",
    "####     Misclassification \n",
    "####     Type-1 and Type-2 error rates\n",
    "####     Sensitivity\n",
    "####     Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T07:16:17.880340Z",
     "start_time": "2021-01-24T07:16:05.766388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "expected = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "results = confusion_matrix(expected, predicted)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T07:34:21.354962Z",
     "start_time": "2021-01-24T07:34:21.348943Z"
    }
   },
   "outputs": [],
   "source": [
    "TP = 4\n",
    "TN = 3\n",
    "FP = 2\n",
    "FN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T07:36:53.498730Z",
     "start_time": "2021-01-24T07:36:53.489242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', ((TP + TN) / (TP + TN + FP + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T07:37:47.801415Z",
     "start_time": "2021-01-24T07:37:47.787095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification:  0.3\n"
     ]
    }
   ],
   "source": [
    "print('Misclassification: ', ((FP + FN) /(TP + TN + FP + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T07:38:37.364100Z",
     "start_time": "2021-01-24T07:38:37.356100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity:  0.8\n"
     ]
    }
   ],
   "source": [
    "print('Sensitivity: ', (TP / (TP + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T07:40:08.436126Z",
     "start_time": "2021-01-24T07:40:08.426740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity:  0.6\n"
     ]
    }
   ],
   "source": [
    "print('Specificity: ', (TN / (TN + FP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T07:43:52.874452Z",
     "start_time": "2021-01-24T07:43:52.865664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-1 error rate:  0.4\n",
      "Type-2 error rate:  0.2\n"
     ]
    }
   ],
   "source": [
    "print('Type-1 error rate: ', (FP / (FP + TN)))\n",
    "print('Type-2 error rate: ', (FN / (FN + TP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
